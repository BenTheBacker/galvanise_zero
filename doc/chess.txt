Tried very briefly with chess.

Did a few (small) generations - starts to get worse and randomly with no traction.  Large part is
that the MCTS will return 0.5, since getting into checkmate state is pretty hard with any objective
(randomly moving pieces in the hope of getting a checkmate before n moves is pretty unlikely).
Infact it is around 10% of games.

After those generations it gets progressively worse - where instead of 10% in becomes likes 2%.

So initial thoughts:

* throw away a bunch of games that end in draws at train time.  Expensive, but the network needs to
  learn something - other than legal moves.
* create a large generation size with random play, so that it doesn't get into the network doesn't
  get into a futile state where it starts to be happy just not ponder a long to the end.  Worse
  than random is never ideal.

How did deepmind overcome this?  Well for one there initial period is 25k * 150 samples.  That is
3.75 million samples to start with.  If could produce that amount of data and then throw away 75% I
still end up with more data than what was needed to train reversi and breakthrough combined.

So restarted, instead of 10k samples from random network - will do 250k.  Could just bypass the
network evaluations altogether (the policy/score are random after all) and save several orders of
magnitude in time.  *This* might be best approach.  Create 2 millions sampls, throw away most of
the draws and see what it learns.

I do think things will change once starts getting some traction.

Extending the length of the game (was 100, now 150) improves the randomly play ratio to 40/60 for
win/losses versus draws.

Another approach is start with super tiny network... then that will increase the speed of things
without avoiding evaluations.  Going to do that now.  Looks like largely throttled by CPU
time... well I guess it is good use case for using the slow g3 AWS instances (they have a ton of
CPUs).  50k evaluations per second using a 1080/1080 ti.  That is actually quite good I think.
Which is approximately 1 game (150 moves * 400 evaluations per move) per second.  25k games would
take 7 hours if I was using pure alphago method.  And 6 days to fill the memory buffer.




